---
title: "Worked solution to 'The difference between `panel_logmeanexp` and `logmeanexp`'"
output:
  html_document:
    toc: no
    toc_depth: 3
bibliography: ../sbied.bib
csl: ../ecology.csl
---

```{r knitr-opts,include=FALSE,cache=FALSE,purl=FALSE}
library(knitr)
prefix <- "contacts"
opts_chunk$set(
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=FALSE,
  cache_extra=rand_seed,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
)

```

```{r echo=F}
library(panelPomp)
```

-------------------------

* It turns out that the SMC algorithm implemented by `pfilter()` gives an unbiased Monte Carlo estimate of the likelihood.

* For inferential purposes, we usually work with the log likelihood. 

* Due to Jensen's inequality, SMC has a negative bias as an estimator of the log likelihood, i.e., it systematically underestimates the log likelihood. 

* Usually, the higher the Monte Carlo variance on the likelihood, the larger this bias.

* Thus, lower Monte Carlo variance on the log likelihood is commonly associated with higher estimated log likelihood. 

* Heuristically, products propagate error rapidly. Averaging Monte Carlo replicates over units before taking a product reduces this error propagation. It does not lead to bias in the likelihood estimate since independence over units and Monte Carlo replicates insures that the expected product of the averages is the expected average of the products.

-------------

------------

## Some notation for a more formal investigation

* Let $\hat\lambda_u^{(k)}$ be the $k$th replication of the Monte Carlo log likelihood evaluation for unit $u$.

* Let $\hat L_u^{(k)}=\exp\big\{\hat\lambda_u^{(k)}\big\}$ be the corresponding likelihood.

* Let  $\hat\lambda^{(k)}=\sum_{u=1}^U \lambda_{u}^{(k)}$ be an estimate the log likelihood of the entire data based on replication $k$.

* Let $\hat L^{(k)}=\exp\big\{\hat\lambda^{(k)}\big\}$ be the corresponding estimate of the likelihood.

* Different possible estimates of the actual log likelihood $\lambda=\sum_{u=1}^U \lambda_u$ are

\begin{eqnarray} \hat\lambda^{[1]} &=& \frac{1}{K}\sum_{k=1}^K \hat\lambda^{(k)} 
\\
  \hat\lambda^{[2]} &=& \log \left( \frac{1}{K}\sum_{k=1}^K 
  \exp \big\{\hat \lambda^{(k)} \big\} \right)  
\\
 \hat\lambda^{[3]} &=& \sum_{u=1}^U\frac{1}{K}\sum_{k=1}^K \hat\lambda^{(k)}_u  
\\
  \hat\lambda^{[4]} &=& \sum_{u=1}^U \log \left( \frac{1}{K}\sum_{k=1}^K \exp\big\{\hat \lambda^{(k)}_u \big\} \right)  
\end{eqnarray}

(a) Check that $\hat\lambda^{[1]}$ and $\hat\lambda^{[3]}$ are equal. 
However, they are inconsistent, since $\hat\lambda^{(k)}_u$ is a biased estimate of $\lambda_u$ and the bias does not disappear when we take an average over replicates.

(b) $\hat\lambda^{[2]}$ is the log mean exp of the total log likelihood for all units.

(c) $\hat\lambda^{[4]}$ is the sum of the log mean exp for each unit separately. 

* To compare variances, it is convenient to move back to the likelihood scale:

\begin{eqnarray} 
  \hat L^{[2]} &=& \frac{1}{K}\sum_{k=1}^K \prod_{u=1}^U L^{(k)} 
\\
  \hat L^{[4]} &=& \prod_{u=1}^U \frac{1}{K}\sum_{k=1}^K \hat L^{(k)}_u  
\end{eqnarray}


* @breto19 showed that $\hat L^{[4]}$ is smaller than  $\hat L^{[2]}$.

* In a limit where $U$ and $K$ both grow together, $\hat L^{[4]}$ can be stable while $\hat L^{[2]}$ increases to infinity.

----------

---------

## References
