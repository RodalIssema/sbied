\usepackage[round,authoryear]{natbib}
\usepackage{paralist}
\usepackage{pgfpages}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{color}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{language=C,showstringspaces=false,keywordstyle=\color{blue},commentstyle=\color{mygreen},basicstyle=\footnotesize}

\usefonttheme[onlymath]{serif}

\newcommand\prob[1]{\mathbb{P}\left[{#1}\right]}
\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\E[1]{\expect{#1}}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}
\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta[1]{{\Delta}{#1}}
\newcommand\lik{\mathcal{L}}
\newcommand\loglik{\ell}
\newcommand\equals{{=\,}}
\newcommand\Rlanguage{\textsf{R}\xspace}
\newcommand\data[1]{#1^*}
\newcommand\params{\, ; \,}
\newcommand\profileloglik{\ell_\mathrm{profile}}
\newcommand\Rzero{\mathfrak{R}_0}
\newcommand\argmax{\mathop{\mathrm{argmax}}}
\newcommand\argmin{\mathop{\mathrm{argmin}}}
\newcommand\answer[2]{#1} % to show blank space
\newcommand\eqspace{\quad\quad\quad}
\newcommand\eqskip{\vspace{2.5mm}}
\newcommand\myeq{\hspace{10mm}}
\newcommand\code[1]{\texttt{#1}}
\newcommand\package[1]{\textbf{#1}}

\usepackage{xspace}        
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}

\newcommand\CHAPTER{5}


\setbeamertemplate{footline}[frame number]

\mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}


\begin{document}
% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
  cache=FALSE,
#  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy.opts=list(width.cutoff=60),
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
# render_listings()
options(width = 60) # number of characters in R output before wrapping
line.color <- "red"
plot.color <- "black"
@

<<prelims,echo=F,cache=F>>=
library(pomp)
stopifnot(packageVersion("pomp")>="2.1")
library(tidyverse)
theme_set(theme_bw())

set.seed(1109529108L)
@

\begin{frame}[fragile]
\frametitle{Chapter \CHAPTER. Case study: Measles in large and small towns}
\hspace{3cm} {\large \bf Objectives}
\begin{itemize}
\item To display a published case study using plug-and-play methods with non-trivial model complexities.
\item To show how extra-demographic stochasticity can be modeled.
\item To demonstrate the use of covariates in \package{pomp}.
\item To demonstrate the use of profile likelihood in scientific inference.
\item To discuss the interpretation of parameter estimates.
\item To emphasize the potential need for extra sources of stochasticity in modeling.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Motivation: challenges in inference from disease dynamics I}
\begin{itemize}

\item Understanding, forecasting, managing epidemiological systems increasingly depends on models.
\item Dynamic models can be used to test causal hypotheses.
\item Real epidemiological systems:
\begin{itemize}
    \item are nonlinear
    \item are stochastic
    \item are nonstationary
    \item evolve in continuous time
    \item have hidden variables
    \item can be measured only with (large) error
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}[fragile]
\frametitle{Motivation: challenges in inference from disease dynamics II}
\begin{itemize}

\item Dynamics of infectious disease outbreaks illustrate this well.

\item Measles is the paradigm for a nonlinear ecological system that can be well described by low-dimensional nonlinear dynamics.
\item A tradition of careful modeling studies have proposed and found evidence for a number of specific mechanisms, including
\begin{itemize}
    \item a high value of $R_0$ (c. 15--20)
    \item under-reporting
    \item seasonality in transmission rates associated with school terms
    \item response to changing birth rates
    \item a birth-cohort effect
    \item metapopulation dynamics
    \item fadeouts and reintroductions that scale with city size
    \item spatial traveling waves
\end{itemize}
\item Much of this evidence has been amassed from fitting models to data, using a variety of methods.
\item See \cite{Rohani2010} for a review of some of the high points.
\end{itemize}
\textcolor{red}{Change eval to T if you want London-Liverpool-Hastings plot}
<<load-data,echo=F,purl=F>>=
daturl <- "https://kingaa.github.io/pomp/vignettes/twentycities.rda"
datfile <- file.path(tempdir(),"twentycities.rda")
download.file(daturl,destfile=datfile,mode="wb")
load(datfile)
@
<<some-data-plot,echo=F,,eval=F,purl=F>>=
measles %>%
  subset(town %in% c("London","Liverpool","Hastings")) %>%
  ggplot(aes(x=date,y=cases))+
  geom_line()+
  facet_grid(town~.,scales='free_y')+
  labs(y="weekly cases")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Outline}
\begin{itemize}
\item We revisit a classic measles data set, weekly case reports in 954 urban centers in England and Wales during the pre-vaccine era (1950--1963).
\item We examine questions regarding:
    \item measles extinction and recolonization
    \item transmission rates
    \item seasonality
    \item resupply of susceptibles
\item We use a model that
\begin{enumerate}
    \item expresses our current understanding of measles dynamics
    \item includes a long list of mechanisms that have been proposed and demonstrated in the literature
    \item cannot be fit by existing likelihood-based methods
\end{enumerate}
\item We examine data from large and small towns using the same model, something no existing methods have been able to do.
\item We ask: does our perspective on this disease change when we expect the models to explain the data in detail?
\item What bigger lessons can we learn regarding inference for dynamical systems?
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{He, Ionides, \& King, \emph{J. R. Soc. Interface} (2010)}

\hspace{3cm} {\large \bf Data sets}
\begin{itemize}
\item Twenty towns, including
\begin{itemize}
    \item 10 largest
    \item 10 smaller, chosen at random
\end{itemize}
\item Population sizes: 2k--3.4M
\item Weekly case reports, 1950--1963
\item Annual birth records and population sizes, 1944--1963
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Map of cities in the analysis}
<<map,echo=F,purl=F>>=
read_csv("data/GB_Coast.csv") %>%
  rename(long=Long,lat=Lat) -> coast

ggplot(mapping=aes(x=long,y=lat))+
  geom_polygon(data=coast,fill=NA,color="blue")+
  geom_point(data=coord,color='red',alpha=1)+
  coord_map(projection="lambert",parameters=c(-2,53))+
  labs(x="",y="")+
  theme_void()
@
\end{frame}

\begin{frame}[fragile]
\frametitle{City case counts I: smallest 8 cities}
<<dataplot,echo=F,fig.height=4,purl=F>>=
demog %>%
  group_by(town) %>%
  summarize(mean.pop=mean(pop)) %>%
  ungroup() %>%
  arrange(mean.pop) -> meanpop

measles %>%
  mutate(town=ordered(town,levels=meanpop$town)) %>%
  filter(town %in% meanpop$town[1:8]) %>%
  ggplot(aes(x=date,y=cases))+
  geom_line()+
  scale_y_continuous(breaks=c(0,4,40,400,4000),trans=scales::log1p_trans())+
  facet_wrap(~town,ncol=2)+theme(text=element_text(size=7))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{City case counts II: largest 8 cities}
<<dataplot2,echo=F,fig.height=4,purl=F>>=
measles %>%
  mutate(town=ordered(town,levels=meanpop$town)) %>%
  filter(town %in% meanpop$town[13:20]) %>%
  ggplot(aes(x=date,y=cases))+
  geom_line()+
  scale_y_continuous(breaks=c(0,4,40,400,4000),trans=scales::log1p_trans())+
  facet_wrap(~town,ncol=2)+theme(text=element_text(size=7))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Continuous-time Markov process model}
<<seir-diagram,echo=FALSE,cache=FALSE,purl=FALSE,eval=FALSE>>=
library(DiagrammeR)
DiagrammeR("digraph SEIR {
  graph [rankdir=TD, overlap=false, fontsize = 10]
  node[shape=egg, label='B'] b;
  subgraph {
    rank=same;
    node[shape=oval, label='S'] S;
    node[shape=oval, label='E'] E;
    node[shape=oval, label='I'] I;
    node[shape=oval, label='R'] R;
    S->E E->I I->R
  }
  node[shape=diamond, label='dead'] d;
  b->S
  {S E I R}->d
   }",type="grViz",engine="dot",height=300,width=800)
@
\includegraphics[]{./model_diagram.png}
\end{frame}

\begin{frame}[fragile]
\frametitle{Continuous-time Markov process model I}
\begin{itemize}
\item Covariates:
\begin{itemize}
    \item $B(t) = \text{birth rate, from data}$
    \item $N(t) = \text{population size, from data}$
\end{itemize}

\item Entry into susceptible class:
$$\mu_{BS}(t) = (1-c)\,B(t-\tau)+c\,\delta(t-\lfloor t\rfloor)\,\int_{t-1}^{t}\,B(t-\tau-s)\,ds$$
\begin{itemize}
	\item $c = \text{cohort effect}$
    \item $\tau = \text{school-entry delay}$
    \item $\lfloor t \rfloor = \text{most recent 1 September before}\ t$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Continuous-time Markov process model II}
\begin{itemize}
\item Force of infection:
$$\mu_{SE}(t) = \tfrac{\beta(t)}{N(t)}\,(I+\iota)\,\zeta(t)$$
\begin{itemize}
    \item $\iota = \text{imported infections}$
    \item $\zeta(t) = \text{Gamma white noise with intensity}\,\sigma_{SE}$ [\cite{He2010,bhadra11}]
    \item school-term transmission:
$$\beta(t) = \begin{cases}\beta_0\,\big(1+a(1-p)/p\big) &\text{during term}\\\beta_0\,(1-a) &\text{during vacation}\end{cases}$$
    \begin{itemize}
    \item $a= \text{amplitude of seasonality}$
    \item $p=0.7589$ is the fraction of the year children are in school.
    \item The factor $(1-p)/p$ ensures that the average transmission rate is $\beta_0$.
    \end{itemize}
\end{itemize}
\item Overdispersed binomial measurement model: $\mathrm{cases}_t\,\vert\,\dlta{N}_{IR}=z_t \sim \dist{Normal}{\rho\,z_t,\rho\,(1-\rho)\,z_t+(\psi\,\rho\,z_t)^2}$
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Implementation in \package{pomp}}
\begin{itemize}
\item We'll load the packages we'll need, and set the random seed, to allow reproducibility.
\item Note that we'll be making heavy use of the \package{tidyverse} methods.
\item Also, we'll be using \package{ggplot2} for plotting: see \href{https://kingaa.github.io/R_Tutorial/viz.html#a-more-systematic-approach-the-grammar-of-graphics}{this brief tutorial}.
\item Finally, we'll use the convenient \package{magrittr} syntax, which is explained \href{https://kingaa.github.io/R_Tutorial/munging.html#the-magrittr-syntax}{here}.
\end{itemize}
<<prelims2,cache=FALSE,echo=F>>=
library(pomp)
library(tidyverse)
theme_set(theme_bw())
set.seed(594709947L)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Data and covariates}
\begin{itemize}
\item Now we'll load the data and covariates.The data are measles reports from 20 cities in England and Wales.
\item We also have information on the population sizes and birth-rates in these cities; we'll treat these variables as covariates.
\item We select the data for London and pre-process the measles and demography data.
\end{itemize}

<<plot-data, echo = F>>=
measles %>%
  mutate(year=as.integer(format(date,"%Y"))) %>%
  filter(town=="London" & year>=1950 & year<1964) %>%
  mutate(
    time=(julian(date,origin=as.Date("1950-01-01")))/365.25+1950
  ) %>%
  filter(time>1950 & time<1964) %>%
  select(time,cases) -> dat

demog %>%
  filter(town=="London") %>%
  select(-town) -> demogLondon
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Data and covariate plots I}
<<data-plot,fig.height=2,echo=F>>=
dat %>% ggplot(aes(x=time,y=cases))+geom_line()

demogLondon %>%
  gather(variable,value,-year) %>%
  ggplot(aes(x=year,y=value))+geom_point()+
  facet_wrap(~variable,ncol=1,scales="free_y")
@
\end{frame}
\begin{frame}[fragile]
\frametitle{Data and covariate plots II}
Now, we smooth the covariates. Note that we delay the entry of newborns into the susceptible pool.
<<prep-covariates,echo=F>>=
demogLondon %>%
  plyr::summarize(
    time=seq(from=min(year),to=max(year),by=1/12),
    pop=predict(smooth.spline(x=year,y=pop),x=time)$y,
    birthrate=predict(smooth.spline(x=year+0.5,y=births),x=time-4)$y
  ) -> covar
@
<<covarplot,fig.height=3.5,echo=F>>=
par(mfrow = c(3,1))
par(mai=c(0.5,0.5,0.2,0.2))
plot(pop~time,data=covar,type='l',xlab="time")
points(pop~year,data=demogLondon)
plot(birthrate~time,data=covar,type='l',xlab="time")
points(births~year,data=demogLondon)
plot(birthrate~I(time-4),data=covar,type='l',xlab="I(time-4)")
points(births~I(year+0.5),data=demogLondon)
# # Top panel
# par(mai=c(0.2,0,0,0))
# plot(pop~time,data=covar,type='l',xaxt="n",yaxt="y",xlab="")
# #axis(1, labels = FALSE)
# points(pop~year,data=demogLondon)
# # Middle panel
# par(mai=c(0.2,0,0,0))
# plot(birthrate~time,data=covar,type='l',xaxt="n",xlab="")
# #axis(1, labels = FALSE)
# points(births~year,data=demogLondon)
# # Bottom panel
# par(mai=c(0,0,0,0))
# plot(birthrate~I(time-4),data=covar,type='l')
# points(births~I(year+0.5),data=demogLondon)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{The partially observed Markov process model}

We require a simulator for the BSEIR model. Notable complexities include:
\begin{enumerate}
\item Incorporation of the known birthrate.
\item The birth-cohort effect: a specified fraction (\code{cohort}) of the cohort enter the susceptible pool all at once.
\item Seasonality in the transmission rate: during school terms, the transmission rate is higher than it is during holidays.
\item Extra-demographic stochasticity in the form of a Gamma white-noise term acting multiplicatively on the force of infection.
\item Demographic stochasticity implmented using Euler-multinomial distributions.
\end{enumerate}
\end{frame}

<<rprocess, echo = FALSE, tidy = TRUE>>=
rproc <- Csnippet("
  double beta, br, seas, foi, dw, births;
  double rate[6], trans[6];

  // cohort effect
  if (fabs(t-floor(t)-251.0/365.0) < 0.5*dt)
    br = cohort*birthrate/dt + (1-cohort)*birthrate;
  else
  	br = (1.0-cohort)*birthrate;

  // term-time seasonality
  t = (t-floor(t))*365.25;
  if ((t>=7&&t<=100) ||
      (t>=115&&t<=199) ||
      (t>=252&&t<=300) || (t>=308&&t<=356))
      seas = 1.0+amplitude*0.2411/0.7589;
  else
      seas = 1.0-amplitude;

  // transmission rate
  beta = R0*(gamma+mu)*seas;
  // expected force of infection
  foi = beta*pow(I+iota,alpha)/pop;
  // white noise (extrademographic stochasticity)
  dw = rgammawn(sigmaSE,dt);
  rate[0] = foi*dw/dt;  // stochastic force of infection
  rate[1] = mu;			    // natural S death
  rate[2] = sigma;		  // rate of ending of latent stage
  rate[3] = mu;			    // natural E death
  rate[4] = gamma;		  // recovery
  rate[5] = mu;			    // natural I death

  // Poisson births
  births = rpois(br*dt);

  // transitions between classes
  reulermultinom(2,S,&rate[0],dt,&trans[0]);
  reulermultinom(2,E,&rate[2],dt,&trans[2]);
  reulermultinom(2,I,&rate[4],dt,&trans[4]);

  S += births   - trans[0] - trans[1];
  E += trans[0] - trans[2] - trans[3];
  I += trans[2] - trans[4] - trans[5];
  R = pop - S - E - I;
  W += (dw - dt)/sigmaSE;  // standardized i.i.d. white noise
  C += trans[4];           // true incidence
")
@

\begin{frame}[fragile,allowframebreaks]
\frametitle{Implementation of the process model}
\begin{lstlisting}
  double beta, br, seas, foi, dw, births;
  double rate[6], trans[6];
  
  // cohort effect
  if (fabs(t-floor(t)-251.0/365.0) < 0.5*dt)
    br = cohort*birthrate/dt + (1-cohort)*birthrate;
  else
    br = (1.0-cohort)*birthrate;

  // term-time seasonality
  t = (t-floor(t))*365.25;
  if ((t>=7 && t<=100) ||
      (t>=115 && t<=199) ||
      (t>=252 && t<=300) ||
      (t>=308 && t<=356))
      seas = 1.0+amplitude*0.2411/0.7589;
  else
      seas = 1.0-amplitude;

  
  // transmission rate
  beta = R0*(gamma+mu)*seas;

  // expected force of infection
  foi = beta*pow(I+iota,alpha)/pop;
  
  // white noise (extrademographic stochasticity)
  dw = rgammawn(sigmaSE,dt);
  
  rate[0] = foi*dw/dt;  // stochastic force of infection
  rate[1] = mu;         // natural S death
  rate[2] = sigma;      // rate of ending of latent stage
  rate[3] = mu;         // natural E death
  rate[4] = gamma;      // recovery
  rate[5] = mu;         // natural I death

  // Poisson births
  births = rpois(br*dt);
  
  // transitions between classes
  reulermultinom(2,S,&rate[0],dt,&trans[0]);
  reulermultinom(2,E,&rate[2],dt,&trans[2]);
  reulermultinom(2,I,&rate[4],dt,&trans[4]);

  S += births   - trans[0] - trans[1];
  E += trans[0] - trans[2] - trans[3];
  I += trans[2] - trans[4] - trans[5];
  R = pop - S - E - I;
  W += (dw - dt)/sigmaSE;  // standardized i.i.d. white noise
  C += trans[4];           // true incidence
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
\frametitle{Implementation of the process model IV}
\begin{itemize}
\item The above code is in the \code{C} programming language. As shown in the remaining components (e.g. \code{rinit} below), we create an \code{R} string with these codes and pass the resulting string to the \code{Csnippet} function.
\item In the above, the variable named \code{C} represents the true incidence, i.e., the number of new infections occurring over an interval.
\item Since recognized measles infections are quarantined, we argue that most infection occurs before case recognition so that true incidence is a measure of the number of individuals progressing from the I to the R compartment in a given interval.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{State initializations}
We complete the process model definition by specifying the distribution of initial unobserved states.
The following codes assume that the fraction of the population in each of the four compartments is known.
<<initializer>>=
rinit <- Csnippet("
  double m = pop/(S_0+E_0+I_0+R_0);
  S = nearbyint(m*S_0);
  E = nearbyint(m*E_0);
  I = nearbyint(m*I_0);
  R = nearbyint(m*R_0);
  W = 0;
  C = 0;
")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{The measurement model I}
\begin{itemize}
\item We'll model both under-reporting and measurement error.
\item We want $\mathbb{E}[\text{cases}|C] = \rho\,C$, where $C$ is the true incidence and $0<\rho<1$ is the reporting efficiency.
\item We'll also assume that $\mathrm{Var}[\text{cases}|C] = \rho\,(1-\rho)\,C + (\psi\,\rho\,C)^2$, where $\psi$ quantifies overdispersion.
\item Note that when $\psi=0$, the variance-mean relation is that of the binomial distribution.
To be specific, we'll choose
$\text{cases|C} \sim f(\cdot|\rho,\psi,C)$, where
\begin{align*}
      f(c|\rho,\psi,C) \\
        =&\Phi(c+\tfrac{1}{2},\rho\,C,\rho\,(1-\rho)\,C+(\psi\,\rho\,C)^2)-\\
        &\Phi(c-\tfrac{1}{2},\rho\,C,\rho\,(1-\rho)\,C+(\psi\,\rho\,C)^2)
    \end{align*}

    %$$f(c|\rho,\psi,C) = \Phi(c+\tfrac{1}{2},\rho\,C,\rho\,(1-\rho)\,C+(\psi\,\rho\,C)^2)-\Phi(c-\tfrac{1}{2},\rho\,C,\rho\,(1-\rho)\,C+(\psi\,\rho\,C)^2),$$
where $\Phi(x,\mu,\sigma^2)$ is the c.d.f. of the normal distribution with mean $\mu$ and variance $\sigma^2$.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The measurement model II}
The following computes $\mathbb{P}[\text{cases}|C]$.
<<dmeasure>>=
dmeas <- Csnippet("
  double m = rho*C;
  double v = m*(1.0-rho+psi*psi*m);
  double tol = 1.0e-18;
  if (cases > 0.0) {
    lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)-pnorm(cases-0.5,m,sqrt(v)+tol,1,0)+tol;
  } else {
    lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)+tol;
  }
")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Case simulations}
The following codes simulate $\text{cases} | C$.
<<rmeasure>>=
rmeas <- Csnippet("
  double m = rho*C;
  double v = m*(1.0-rho+psi*psi*m);
  double tol = 1.0e-18;
  cases = rnorm(m,sqrt(v)+tol);
  if (cases > 0.0) {
    cases = nearbyint(cases);
  } else {
    cases = 0.0;
  }
")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Constructing the `pomp` object}
<<pomp-construction>>=
dat %>%
  pomp(t0=with(dat,2*time[1]-time[2]),
    time="time",
    rprocess=euler(rproc,delta.t=1/365.25),
    rinit=rinit,
    dmeasure=dmeas,
    rmeasure=rmeas,
    covar=covariate_table(covar,times="time"),
    accumvars=c("C","W"),
    statenames=c("S","E","I","R","C","W"),
    paramnames=c("R0","mu","sigma","gamma","alpha","iota",
      "rho","sigmaSE","psi","cohort","amplitude",
      "S_0","E_0","I_0","R_0")
  ) -> m1
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Estimates from \cite{He2010}}
\cite{He2010} estimated the parameters of this model.
The full set is included in the \Rlanguage code accompanying this document, where they are read into a data frame called `mles`.
<<mles,include=FALSE>>=
read.csv(text="
town,loglik,loglik.sd,mu,delay,sigma,gamma,rho,R0,amplitude,alpha,iota,cohort,psi,S_0,E_0,I_0,R_0,sigmaSE
Bedwellty,-1125.1,0.14,0.02,4,57.9,146,0.311,24.7,0.16,0.937,0.0396,0.351,0.951,0.0396,2.64e-05,2.45e-05,0.96,0.0611
Birmingham,-3239.3,1.55,0.02,4,45.6,32.9,0.544,43.4,0.428,1.01,0.343,0.331,0.178,0.0264,8.96e-05,0.000335,0.973,0.0611
Bradford,-2586.6,0.68,0.02,4,45.6,129,0.599,32.1,0.236,0.991,0.244,0.297,0.19,0.0365,7.41e-06,4.59e-06,0.964,0.0451
Bristol,-2681.6,0.5,0.02,4,64.3,82.6,0.626,26.8,0.203,1.01,0.441,0.344,0.201,0.0358,9.62e-06,5.37e-06,0.964,0.0392
Cardiff,-2364.9,0.73,0.02,4,39,143,0.602,34.4,0.223,0.996,0.141,0.267,0.27,0.0317,1.01e-05,9.21e-06,0.968,0.0539
Consett,-1362.9,0.73,0.02,4,42.6,172,0.65,35.9,0.2,1.01,0.0731,0.31,0.406,0.0322,1.83e-05,1.97e-05,0.968,0.0712
Dalton.in.Furness,-726.1,0.3,0.02,4,73.6,257,0.455,28.3,0.203,0.989,0.0386,0.421,0.818,0.0387,2.23e-05,2.36e-05,0.961,0.0779
Halesworth,-318.6,0.51,0.02,4,49.6,210,0.754,33.1,0.381,0.948,0.00912,0.547,0.641,0.0526,1.99e-05,2.82e-05,0.947,0.0748
Hastings,-1583.7,0.21,0.02,4,56.3,74.1,0.695,34.2,0.299,1,0.186,0.329,0.396,0.0233,5.61e-06,3.4e-06,0.977,0.0955
Hull,-2729.4,0.39,0.02,4,42.1,73.9,0.582,38.9,0.221,0.968,0.142,0.275,0.256,0.0371,1.2e-05,1.13e-05,0.963,0.0636
Leeds,-2918.6,0.23,0.02,4,40.7,35.1,0.666,47.8,0.267,1,1.25,0.592,0.167,0.0262,6.04e-05,3e-05,0.974,0.0778
Lees,-548.1,1.1,0.02,4,45.6,244,0.612,29.7,0.153,0.968,0.0311,0.648,0.681,0.0477,2.66e-05,2.08e-05,0.952,0.0802
Liverpool,-3403.1,0.34,0.02,4,49.4,39.3,0.494,48.1,0.305,0.978,0.263,0.191,0.136,0.0286,0.000184,0.00124,0.97,0.0533
London,-3804.9,0.16,0.02,4,28.9,30.4,0.488,56.8,0.554,0.976,2.9,0.557,0.116,0.0297,5.17e-05,5.14e-05,0.97,0.0878
Manchester,-3250.9,0.66,0.02,4,34.4,56.8,0.55,32.9,0.29,0.965,0.59,0.362,0.161,0.0489,2.41e-05,3.38e-05,0.951,0.0551
Mold,-296.5,0.25,0.02,4,67.4,301,0.131,21.4,0.271,1.04,0.0145,0.436,2.87,0.064,2.61e-05,2.27e-05,0.936,0.0544
Northwich,-1195.1,2.25,0.02,4,45.6,147,0.795,30.1,0.423,0.948,0.0602,0.236,0.402,0.0213,1.32e-05,1.58e-05,0.979,0.0857
Nottingham,-2703.5,0.53,0.02,4,70.2,115,0.609,22.6,0.157,0.982,0.17,0.34,0.258,0.05,1.36e-05,1.41e-05,0.95,0.038
Oswestry,-696.1,0.49,0.02,4,37.3,168,0.631,52.9,0.339,1.04,0.0298,0.263,0.476,0.0218,1.56e-05,1.61e-05,0.978,0.0699
Sheffield,-2810.7,0.21,0.02,4,54.3,62.2,0.649,33.1,0.313,1.02,0.853,0.225,0.175,0.0291,6.04e-05,8.86e-05,0.971,0.0428
",stringsAsFactors=FALSE) -> mles
@

<<mle,include=FALSE>>=
mles %>% filter(town=="London") -> mle
paramnames <- c("R0","mu","sigma","gamma","alpha","iota",
  "rho","sigmaSE","psi","cohort","amplitude",
  "S_0","E_0","I_0","R_0")
mle[paramnames] %>% unlist() -> theta
mle %>% select(-S_0,-E_0,-I_0,-R_0)
@

We verify that we get the same likelihood as \cite{He2010}. \textcolor{red}{Change eval to T}

<<pfilter1,eval=F>>=
library(foreach)
library(doParallel)
library(doRNG)

registerDoParallel()
registerDoRNG(998468235L)

foreach(i=1:4) %dopar% {
  library(pomp)
  pfilter(m1,Np=10000,params=theta)
} -> pfs
logmeanexp(sapply(pfs,logLik),se=TRUE)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Simulations at the MLE}
<<sims1,fig.height=2,echo=T>>=
m1 %>%
  simulate(params=theta,nsim=3,format="d",include.data=TRUE) %>%
  ggplot(aes(x=time,y=cases,group=.id,color=(.id=="data")))+
  guides(color=FALSE)+
  geom_line()+facet_wrap(~.id,ncol=2)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Parameter transformations}
\begin{itemize}
\item The parameters are constrained to be positive, and some of them are constrained to lie between $0$ and $1$.

\item We can turn the likelihood maximization problem into an unconstrained maximization problem by transforming the parameters.

\item Specifically, to enforce positivity, we log transform,
to constrain parameters to $(0,1)$, we logit transform,
and to confine parameters to the unit simplex, we use the log-barycentric transformation.
\end{itemize}

<<transforms>>=
pt <- parameter_trans(
  log=c("sigma","gamma","sigmaSE","psi","R0"),
  logit=c("cohort","amplitude"),
  barycentric=c("S_0","E_0","I_0","R_0")
)
m1 %>%
  pomp(partrans=pt,
    statenames=c("S","E","I","R","C","W"),
    paramnames=c("R0","mu","sigma","gamma","alpha","iota",
      "rho","sigmaSE","psi","cohort","amplitude",
      "S_0","E_0","I_0","R_0")) -> m1
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Results from \cite{He2010}}
\href{./profile.html}{The linked document shows how a likelihood profile can be constructed using IF2}
The fitting procedure used is as follows:
\begin{itemize}
\item A large number of searches were started at points across the parameter space.
\item Iterated filtering was used to maximize the likelihood.
\item We obtained point estimates of all parameters for 20 cities.
\item We constructed profile likelihoods to quantify uncertainty in London and Hastings.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Imported infections}
$$\text{force of infection} = \mu_{SE}=\frac{\beta(t)}{N(t)}\,(I+\iota)\,\zeta(t)$$
<<imports,results="hide",echo=FALSE,purl=FALSE>>=
best <- read.csv('data/iota.csv',row.names=1)

op <- par(
  font=2,
  fig=c(0,1,0,1),
  mar=c(4,4,4,4),
  bty='l'
)
plot.new()
mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
mtext(side=1,line=2,text=expression(paste("imported infections, ",iota)),adj=0.5)

x <- best[grep('London',rownames(best)),]
fit <- loess(loglik~log(iota),data=x,span=0.7)
nd <- data.frame(iota=with(x,exp(seq(from=min(log(iota)),to=max(log(iota)),length=100))))
nd$loglik <- predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
conf.int <- range(nd$iota[nd$loglik>cutoff],na.rm=T)
par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
plot(
  loglik~iota,
  data=x,
  font=2,
  bty='l',
  ann=F,
  xaxt='n',
  log='x',
  xlim=c(0.001,100),
  ylim=max(x$loglik)+c(-15,1)
)
axis(side=1,at=c(0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100),labels=F)
lines(loglik~iota,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')
text(0.002,cutoff,"London",pos=3)

x <- best[grep('Hastings',rownames(best)),]
fit <- loess(loglik~log(iota),data=x,span=0.7)
nd <- data.frame(iota=with(x,seq(from=min(iota),to=max(iota),length=100)))
nd$loglik <- predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
conf.int <- range(nd$iota[nd$loglik>cutoff],na.rm=T)
par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
plot(
  loglik~iota,
  data=x,
  font=2,
  bty='l',
  ann=F,
  xaxt='n',
  log='x',
  xlim=c(0.001,100),
  ylim=max(x$loglik)+c(-15,1)
)
axis(
  side=1,
  at=c(0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100),
  labels=c(expression(10^-3),"",expression(10^-2),"",expression(10^-1),"",expression(10^0),"",expression(10^1),"",expression(10^2))
)
lines(loglik~iota,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')
text(0.002,cutoff,"Hastings",pos=3)
par(op)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Seasonality}
<<amplitude,results="hide",echo=FALSE,purl=FALSE>>=
best <- read.csv('data/amplitude.csv',row.names=1)

op <- par(
  font=2,
  fig=c(0,1,0,1),
  mar=c(4,4,4,4),
  bty='l'
)
plot.new()
mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
mtext(side=1,line=2,text="amplitude of term-time seasonality",adj=0.5)

x <- best[grep('London',rownames(best)),]
fit <- loess(loglik~amplitude,data=x,span=0.7)
nd <- data.frame(amplitude=with(x,seq(from=min(amplitude),to=max(amplitude),length=100)))
nd$loglik <- predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
conf.int <- range(nd$amplitude[nd$loglik>cutoff],na.rm=T)
par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
plot(
  loglik~amplitude,
  data=x,
  font=2,
  bty='l',
  ann=F,
  xaxt='n',
  xlim=c(0,1),
  ylim=max(x$loglik)+c(-10,1)
)
axis(side=1,at=seq(0,1,by=0.2),labels=F)
lines(loglik~amplitude,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')
text(0.9,cutoff,"London",pos=3)

x <- best[grep('Hastings',rownames(best)),]
fit <- loess(loglik~amplitude,data=x,span=0.7)
nd <- data.frame(amplitude=with(x,seq(from=min(amplitude),to=max(amplitude),length=100)))
nd$loglik <- predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
conf.int <- range(nd$amplitude[nd$loglik>cutoff],na.rm=T)
par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
plot(
  loglik~amplitude,
  data=x,
  font=2,
  bty='l',
  ann=F,
  xaxt='n',
  xlim=c(0,1),
  ylim=max(x$loglik)+c(-10,1)
)
axis(side=1,at=seq(0,1,by=0.2))
lines(loglik~amplitude,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')
text(0.9,cutoff,"Hastings",pos=3)
par(op)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Notable findings}
\hspace{3cm} {\large \bf Cohort effect}
<<cohort-effect,echo=FALSE,results="hide",purl=FALSE>>=
best <- read.csv('data/cohort.csv',row.names=1)

op <- par(
  font=2,
  fig=c(0,1,0,1),
  mar=c(4,4,4,4),
  bty='l'
)
plot.new()
mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
mtext(side=1,line=2,text="cohort entry fraction",adj=0.5)

x <- best[grep('London',rownames(best)),]
fit <- loess(loglik~cohort,data=x,span=0.7)
nd <- data.frame(cohort=with(x,seq(from=min(cohort),to=max(cohort),length=100)))
nd$loglik <- predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
conf.int <- range(nd$cohort[nd$loglik>cutoff],na.rm=T)
par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
plot(
  loglik~cohort,
  data=x,
  font=2,
  bty='l',
  ann=F,
  xaxt='n',
  xlim=c(0,1),
  ylim=max(x$loglik)+c(-10,1)
)
axis(side=1,at=seq(0,1,by=0.2),labels=F)
lines(loglik~cohort,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')
text(0.1,cutoff,"London",pos=3)

x <- best[grep('Hastings',rownames(best)),]
fit <- loess(loglik~cohort,data=x,span=0.7)
nd <- data.frame(cohort=with(x,seq(from=min(cohort),to=max(cohort),length=100)))
nd$loglik <- predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
conf.int <- range(nd$cohort[nd$loglik>cutoff],na.rm=T)
par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
plot(
  loglik~cohort,
  data=x,
  font=2,
  bty='l',
  ann=F,
  xaxt='n',
  xlim=c(0,1),
  ylim=max(x$loglik)+c(-10,1)
)
axis(side=1,at=seq(0,1,by=0.2))
lines(loglik~cohort,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')
text(0.1,cutoff,"Hastings",pos=1)
par(op)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Notable findings}
\hspace{3cm} {\large \bf Birth delay}
<<delay,eval=TRUE,echo=FALSE,results="hide",purl=FALSE,fig.height=3,fig.cap="Profile likelihood for birth-cohort delay, showing 95\\% and 99\\% critical values of the log likelihood.">>=
x <- read.csv('data/delay.csv',row.names=1)
par(mai=c(0.2,0.3,0.2,0.2))
plot(
  x$delay,
  x$loglik,
  type='o',
  xlab=expression(paste(tau,' (yr)')),
  ylab='profile log likelihood'
)
abline(h=max(x$loglik)-0.5*qchisq(p=c(0.95,0.99),df=1),lty='33')
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Notable findings}
\hspace{3cm} {\large \bf Reporting rate}
<<report-rate,echo=FALSE,results="hide",purl=FALSE>>=
mles %>% select(town,rho) -> est.rho

demog %>%
  subset(year==1950) %>%
  select(town,pop) -> pop

measles %>%
  mutate(year=as.integer(format(date,"%Y"))) %>%
  group_by(town,year) %>%
  summarize(cases=sum(cases)) %>%
  ungroup() %>%
  left_join(demog,by=c("town","year")) %>%
  group_by(town) %>%
  transmute(
    cases=cumsum(cases),
    births=cumsum(births)
  ) %>%
  ungroup() -> m

m %>%
  group_by(town) %>%
  do({
    fit <- lm(cases~births,data=.)
    data.frame(slope=fit$coefficients[2])
  }) %>%
  left_join(pop,by='town') %>%
  left_join(est.rho,by='town') %>%
  rename(regression=slope,model=rho) %>%
  gather(variable,value,-town,-pop) %>%
  ggplot(aes(x=town,y=value,shape=variable,group=town))+
  geom_point(size=3)+
  geom_line(alpha=0.5)+
  labs(x="",y="estimated reporting rate",
    variable="estimate")+
  theme(axis.text.x=element_text(angle=90,hjust=1))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Notable findings}
\hspace{3cm} {\large \bf Predicted vs observed critical community size}
<<fadeouts,echo=FALSE,results="hide",purl=FALSE>>=
op <- par(
  font=2,
  fig=c(0,1,0,1),
  mar=c(4,4,4,4),
  bty='l'
)
ccssim <- read.csv('data/ccssim.csv',comment.char='#')
ccsdata <- read.csv('data/fadeouts_954.csv',row.names=1,comment.char='#')
plot(
  prop.fadeout~mean.pop,
  data=ccsdata,
  bty='l',
  log='x',
  ann=F,
  pch=20,
  xlim=range(ccssim$pop),
  ylim=c(0,1)
)
lines(prop.fadeout~pop,data=ccssim,lwd=3,col=line.color)
mtext(side=1,line=3,text='community size')
mtext(side=2,line=3,text='proportion of weeks without cases')
par(op)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Problematic results}
\hspace{3cm} {\large \bf $R_0$ estimates inconsistent with literature}
\begin{itemize}
\item Recall that $R_0$ : a measure of how communicable an infection is.
\item Existing estimates of $R_0$ (c. 15--20) come from two sources: serology surveys, and models fit to data using feature-based methods.
\end{itemize}
<<R0,echo=FALSE,results="hide",purl=FALSE,fig.height=3>>=
best <- read.csv('data/R0profile.csv',row.names=1)

op <- par(
  font=2,
  fig=c(0,1,0,1),
  mar=c(4,4,4,4),
  bty='l'
)
plot.new()
mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
mtext(side=1,line=2,text=expression(R[0]),adj=0.5)

x <- best[grep('London',rownames(best)),]
fit <- loess(loglik~R0,data=x,span=0.7)
nd <- data.frame(R0=with(x,seq(from=min(R0),to=max(R0),length=100)))
nd$loglik <- predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
conf.int <- range(nd$R0[nd$loglik>cutoff],na.rm=T)
par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
plot(
  loglik~R0,
  data=x,
  font=2,
  bty='l',
  ann=F,
  xaxt='n',
  xlim=c(10,100),
  ylim=max(x$loglik)+c(-10,1)
)
axis(side=1,at=seq(10,100,by=30),labels=F)
lines(loglik~R0,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')
text(90,max(x$loglik),"London")

x <- best[grep('Hastings',rownames(best)),]
fit <- loess(loglik~R0,data=x,span=0.7)
nd <- data.frame(R0=with(x,seq(from=min(R0),to=max(R0),length=100)))
nd$loglik <- predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
conf.int <- range(nd$R0[nd$loglik>cutoff],na.rm=T)
par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
plot(
  loglik~R0,
  data=x,
  font=2,
  bty='l',
  ann=F,
  xaxt='n',
  xlim=c(10,100),
  ylim=max(x$loglik)+c(-10,1)
)
axis(side=1,at=seq(10,100,by=30))
lines(loglik~R0,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')
text(90,max(x$loglik),"Hastings")
par(op)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Problematic results}
\hspace{3cm} {\large \bf Parameter estimates}
<<est-table,echo=FALSE,size="scriptsize",purl=FALSE>>=
mean.euler <- function(rate,dt=1/365) dt/(1-exp(-rate*dt))

demog %>%
  filter(year==1950) %>%
  select(town,pop) -> pop

mles %>%
  left_join(pop,by='town') %>%
  mutate(
    IP=365*mean.euler(gamma),
    LP=365*mean.euler(sigma)
  ) %>%
  arrange(pop) %>%
  select(town,pop,R0,amplitude,LP,IP,alpha,iota,rho,psi,sigmaSE) -> est

est %>%
  gather(variable,value,-town,-pop) %>%
  group_by(variable) %>%
  summarize(
    cor=cor(log(value),log(pop),use="all.obs",method="spearman")
  ) -> cors
cors <- setNames(cors$cor,cors$variable)

est %>%
  magrittr::set_rownames(est$town) %>%
  subset(select=-town) -> tab
tab %>%
  rbind(r=cors[names(tab)]) %>%
  signif(3)
@

$r = \mathrm{cor}(\log{\hat\theta},\log{N_{1950}})$

\end{frame}

\begin{frame}[fragile]
\frametitle{Problematic results}
\hspace{3cm} {\large \bf Extrademographic stochasticity}
$$\mu_{SE}=\frac{\beta(t)}{N(t)}\,(I+\iota)\,\zeta(t)$$
<<env-noise,echo=FALSE,results="hide",purl=FALSE>>=
best <- read.csv('data/env_noise.csv',row.names=1)
mean.euler <- function(rate,dt=1/365) dt/(1-exp(-rate*dt))
best$IP <- mean.euler(best[,'gamma'])*365
best$LP <- mean.euler(best[,'sigma'])*365
best$sigSE <- 1.0/sqrt(best[,'eta'])

op <- par(
  font=2,
  fig=c(0,1,0,1),
  mar=c(4,4,4,4)
)
plot.new()
mtext(side=1,line=2,text=expression(sigma[SE]),adj=0.5)
mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
mtext(side=4,line=2.5,text="duration (days)",adj=0.5)

x <- best[grep('London',rownames(best)),]

fit <- loess(loglik~sigSE,data=x,span=0.7)
nd <- data.frame(sigSE=with(x,seq(from=min(sigSE),to=max(sigSE),length=100)))
nd$loglik <- predict(fit,newdata=nd)
fit <- loess(IP~sigSE,data=x,span=0.7)
nd$IP<-predict(fit,newdata=nd)
fit <- loess(LP~sigSE,data=x,span=0.7)
nd$LP<-predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
ci.lond <- conf.int <- range(nd$sigSE[nd$loglik>cutoff],na.rm=T)

par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
plot(
  loglik~sigSE,
  data=x,
  bty='u',
  ann=F,
  xaxt='n',
  yaxt='n',
  log='x',
  xlim=c(0.005,0.5),
  ylim=max(x$loglik)+c(-10,1)
)
axis(side=1,at=c(0.005,0.01,0.02,0.05,0.1,0.2,0.5),labels=F)
axis(side=2)
lines(loglik~sigSE,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')

IPcol='red'
LPcol='blue'
IPlty=1
LPlty=1
plot.window(xlim=c(0.005,0.5),ylim=c(0,20))
lines(IP~sigSE,data=nd,lwd=2,col=IPcol,lty=IPlty)
lines(LP~sigSE,data=nd,lwd=2,col=LPcol,lty=LPlty)
axis(side=4)

plot.window(xlim=c(1,10),ylim=c(0,1))
text(10^(0.9),0.8,"London",pos=3)
legend("topleft",lwd=2,col=c(IPcol,LPcol),
  lty=c(IPlty,LPlty),legend=c("IP","LP"),bty='n',bg='white')


x <- best[grep('Hastings',rownames(best)),]

fit <- loess(loglik~sigSE,data=x,span=0.7)
nd <- data.frame(sigSE=with(x,seq(from=min(sigSE),to=max(sigSE),length=100)))
nd$loglik <- predict(fit,newdata=nd)
fit <- loess(IP~sigSE,data=x,span=0.7)
nd$IP<-predict(fit,newdata=nd)
fit <- loess(LP~sigSE,data=x,span=0.7)
nd$LP<-predict(fit,newdata=nd)
cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
ci.hast <- conf.int <- range(nd$sigSE[nd$loglik>cutoff],na.rm=T)

par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
plot(
  loglik~sigSE,
  data=x,
  bty='l',
  ann=F,
  xaxt='n',
  yaxt='n',
  log='x',
  bty='u',
  xlim=c(0.005,0.5),
  ylim=max(x$loglik)+c(-10,1)
)
axis(side=1,at=c(0.005,0.01,0.02,0.05,0.1,0.2,0.5))
axis(side=2)
lines(loglik~sigSE,data=nd,col=line.color)
abline(h=cutoff,lty='33')
abline(v=conf.int,lty='63')

plot.window(xlim=c(0.005,0.5),ylim=c(0,20))
lines(IP~sigSE,data=nd,lwd=2,col=IPcol,lty=IPlty)
lines(LP~sigSE,data=nd,lwd=2,col=LPcol,lty=LPlty)
axis(side=4)

plot.window(xlim=c(1,10),ylim=c(0,1))
text(10^(0.1),0.8,"Hastings",pos=3)

par(op)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Questions}
\begin{itemize}
\item What does it mean that parameter estimates from the fitting disagree with estimates from other data?
\item How can one interpret the correlation between infectious period and city size in the parameter estimates?
\item How do we interpret the need for extrademographic stochasticity in this model?
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simulations at the MLE}
<<sims2,echo=F,fig.height=2>>=
m1 %>%
  simulate(params=theta,nsim=100,format="d",include.data=TRUE) %>%
  select(time,.id,cases) -> simdat

simdat %>%
  mutate(data=.id=="data") %>%
  plyr::ddply(~time+data,plyr::summarize,
    p=c(0.05,0.5,0.95),
    q=quantile(cases,prob=p,names=FALSE)
  ) %>%
  mutate(p=plyr::mapvalues(p,from=c(0.05,0.5,0.95),to=c("lo","med","hi")),
         data=plyr::mapvalues(data,from=c(TRUE,FALSE),to=c("data","simulation"))) %>%
  spread(p,q) %>%
  ggplot(aes(x=time,y=med,color=data,fill=data,ymin=lo,ymax=hi))+
  geom_ribbon(alpha=0.2)+
  guides(data=FALSE)

simdat %>%
  filter(.id=="data" | .id <= "5") %>%
  mutate(data=.id=="data") %>%
  ggplot(aes(x=time,y=cases,group=.id,color=data))+
  geom_line()+
  guides(color=FALSE)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Exercises}
\hspace{3cm} {\large \bf Exercise 1: Reformulate the model}
\begin{itemize}
\item Modify the \cite{He2010} model to remove the cohort effect.
Run simulations and compute likelihoods to convince yourself that the resulting codes agree with the original ones for `cohort = 0`.
\item Now modify the transmission seasonality to use a sinusoidal form. How many parameters must you use?
Fixing the other parameters at their MLE values, compute and visualize a profile likelihood over these parameters.
\end{itemize}
\hspace{3cm} {\large \bf Exercise 2: Extrademographic stochasticity}
Set the extrademographic stochasticity parameter $\sigma_{SE}=0$, set $\alpha=1$, and fix $\rho$ and $\iota$ at their MLE values, then maximize the likelihood over the remaining parameters.
\begin{itemize}
\item
How do your results compare with those at the MLE? Compare likelihoods but also use simulations to diagnose differences between the models.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Acknowledgments}
\begin{itemize}
\item Versions of this material have been presented at \href{https://www.biostat.washington.edu/suminst/sismid}{Summer Institute in Statistics and Modeling in Infectious Diseases (SISMID)} in 2015-2018.

\item Licensed under the \href{Creative Commons Attribution-NonCommercial license}{http://creativecommons.org/licenses/by-nc/4.0/}.
Please share and remix noncommercially, mentioning its origin.
\includegraphics[]{../graphics/cc-by-nc.png}
\item Produced in \Rlanguage version \Sexpr{getRversion()} using \package{pomp} version \Sexpr{packageVersion('pomp')}.

\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{References}
  \bibliographystyle{dcu}
  \bibliography{../sbied.bib}
\end{frame}

\end{document}
