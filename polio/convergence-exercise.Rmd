---
title: "Diagnosing filtering and maximization convergence for the polio case study"
author: "Edward L. Ionides and Aaron A. King"
output:
  html_document:
    toc: no
bibliography: ../sbied.bib
csl: ../ecology.csl
---


-----------------------------------

```{r knitr-opts,include=FALSE,purl=FALSE}
library(knitr)
prefix <- "nelder"
opts_chunk$set(
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=3,fig.width=4.85,
  dpi=100,
  dev='png',
  dev.args=list(bg='transparent')
  )
```
```{r prelims,include=FALSE,cache=FALSE}
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  encoding="UTF-8"
  )

set.seed(594709947L)
library(ggplot2)
theme_set(theme_bw())
library(plyr)
library(reshape2)
library(magrittr)
library(pomp)
stopifnot(packageVersion("pomp")>"2.0.9")
```

\newcommand\prob[1]{\mathbb{P}\left[{#1}\right]}
\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}
\newcommand\cov[2]{\mathrm{Cov}\left[{#1},{#2}\right]}
\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dd[1]{d{#1}}
\newcommand\dlta[1]{{\Delta}{#1}}
\newcommand\lik{\mathcal{L}}
\newcommand\loglik{\ell}
\newcommand\R{\mathbb{R}}
\newcommand\data[1]{#1^*}
\newcommand\params{\, ; \,}
\newcommand\transpose{\scriptsize{T}}
\newcommand\eqspace{\quad\quad\quad}
\newcommand\profileloglik[1]{\ell^\mathrm{profile}_#1}


### Exercise

Are there outliers in the data (i.e., observations that do not fit well with our model)? Are we using unnecessarily large amounts of computer time to get our results? Are there indications that we would should run our computations for longer? Or maybe with different choices of algorithmic settings?
Interpret the diagnostic plots below, and suggest things to try that might lead to a more effective search.


<br>

--------

---------

### Solution

The effective sample size decreases to around 10 at several points later in the time series. at least 100 is more comfortable. If this issue is not resolved later in the search, more particles may be needed. 

The log likelihood is trending upwards, suggesting that continuing the search longer is worthwhile. 

The convergence plots do not show stabilization to the point where each search has become localized. Increasing the number of iterations, allowing for further decrease in the temperature, may allow the trajectories to level out. 

Although the searches are investigating differing regions of parameter space, their likelihood are fairly comparable, up to Monte Carlo uncertainty, so that is not a major concern. There may be some weakly identified combination of parameters, or just considerable Monte Carlo error. In such situations, the search results should be considered a collection of points with likelihood near the MLE rather than precise estimates of the exact MLE.


--------------------------

-----------------------------------

[Licensed under the Creative Commons Attribution-NonCommercial license](http://creativecommons.org/licenses/by-nc/4.0/).
Please share and remix noncommercially, mentioning its origin.  
![CC-BY_NC](../graphics/cc-by-nc.png)

This version produced in R `r getRversion()` on `r format(Sys.Date(), "%B %d, %Y")`.




